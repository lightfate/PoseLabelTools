{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/home/ubuntu/zzx/AlphaPose/SPPE/src/utils/img.py:11: UserWarning: \n",
      "This call to matplotlib.use() has no effect because the backend has already\n",
      "been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
      "or matplotlib.backends is imported for the first time.\n",
      "\n",
      "The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 486, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 127, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/asyncio/base_events.py\", line 422, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/asyncio/base_events.py\", line 1432, in _run_once\n",
      "    handle._run()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/asyncio/events.py\", line 145, in _run\n",
      "    self._callback(*self._args)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 117, in _handle_events\n",
      "    handler_func(fileobj, events)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n",
      "    raw_cell, store_history, silent, shell_futures)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2903, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-1-2f11ef94a804>\", line 6, in <module>\n",
      "    from myutil import *\n",
      "  File \"/home/ubuntu/zzx/mymodule/myutil.py\", line 42, in <module>\n",
      "    import matplotlib.pyplot as plt\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/matplotlib/pyplot.py\", line 71, in <module>\n",
      "    from matplotlib.backends import pylab_setup\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/matplotlib/backends/__init__.py\", line 16, in <module>\n",
      "    line for line in traceback.format_stack()\n",
      "\n",
      "\n",
      "  matplotlib.use('agg')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pose model from ./models/sppe/duc_se.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2384 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/modules/upsampling.py:129: UserWarning: nn.Upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.{} is deprecated. Use nn.functional.interpolate instead.\".format(self.name))\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:2351: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n",
      "100%|██████████| 2384/2384 [16:40<00:00,  6.01it/s]\n",
      "2384it [00:07, 321.24it/s]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import glob,os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from myutil import *\n",
    "\n",
    "from mydataloader import FrameProcessor, YoloLoader, PoseModelLoader\n",
    "\n",
    "class myVideoLoader(object):\n",
    "    def __init__(self, videoPath):\n",
    "        self.videoPath = videoPath\n",
    "        self.dirPath , self.videoname = os.path.split(videoPath)\n",
    "        self.cap = cv2.VideoCapture(videoPath)\n",
    "        self.fps = int(self.cap.get(cv2.CAP_PROP_FPS)) \n",
    "        self.fcount = int(self.cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        \n",
    "    def get_audio(self):\n",
    "        cap = cv2.VideoCapture(self.videoPath)\n",
    "        ret, frame = cap.read()\n",
    "        self.audioPath = self.videoPath[:-4]+'.wav'\n",
    "        command = \"ffmpeg -i {} -vn -ar 16000 -f wav {}\".format(self.videoPath, self.audioPath)\n",
    "        try:\n",
    "            subprocess.check_call(command, shell=True)\n",
    "            is_success = True\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(\"error code: {}! shell command: {}\".format(e.returncode, e.cmd))\n",
    "            is_success = False\n",
    "        assert is_success==True, print('convert error!')\n",
    "        cap.release()\n",
    "        return self.audioPath\n",
    "     \n",
    "    \n",
    "    def extract_frame(self, savDir, by_sec=1, useFilter = True, min_det_num = 40, filter_jump_sec=5):\n",
    "        #by_sec=5 : save frame every 5 second \n",
    "        #useFilter, min_det_num : use alphapose to detect person number and filter the frame (detect_num<min_det_num)\n",
    "        #filter_jump_sec : if detect frame, jump x sec.\n",
    "        if not os.path.exists(savDir):\n",
    "            os.makedirs(savDir)\n",
    "            \n",
    "        if useFilter:\n",
    "            # Load model\n",
    "            pose_model_loader = PoseModelLoader()\n",
    "            det_model_loader = YoloLoader()\n",
    "            \n",
    "        cap = cv2.VideoCapture(self.videoPath)\n",
    "        fI_jump=0\n",
    "        for fI in tqdm(range(0,self.fcount,self.fps*by_sec)):\n",
    "            if fI<fI_jump:\n",
    "                continue\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES,fI)\n",
    "            ret, frame = cap.read()\n",
    "            if useFilter:\n",
    "                framePose = FrameProcessor(frame, det_model_loader, pose_model_loader).start()\n",
    "                result = framePose.resultNew\n",
    "                person_num = framePose.det_human_num\n",
    "                if person_num<min_det_num:\n",
    "                    continue\n",
    "            imgSavname=self.videoname[:-4]+'_'+str(fI)+'_'+str(person_num)+'.jpg'\n",
    "            jsonSavname=self.videoname[:-4]+'_'+str(fI)+'_'+str(person_num)+'.json'\n",
    "            sav_img(os.path.join(savDir,self.videoname[:-4],imgSavname), frame)\n",
    "            with open(os.path.join(savDir,self.videoname[:-4],jsonSavname),'w',encoding='utf-8') as jsonfile:\n",
    "                json.dump(result, jsonfile)\n",
    "            fI_jump = fI + self.fps*filter_jump_sec\n",
    "            #print(person_num, savname)\n",
    "        cap.release() \n",
    "    \n",
    "    def auto_extract_frame(self, savDir, by_sec=1, filter_jump_sec=5):\n",
    "        imgSavDir = os.path.join(savDir,self.videoname[:-4],'images')\n",
    "        jsonSavDir = os.path.join(savDir,self.videoname[:-4],'keypoints')\n",
    "        \n",
    "        if not os.path.exists(savDir):\n",
    "            os.makedirs(savDir)\n",
    "        if not os.path.exists(imgSavDir):\n",
    "            os.makedirs(imgSavDir)\n",
    "        if not os.path.exists(jsonSavDir):\n",
    "            os.makedirs(jsonSavDir)\n",
    "        \n",
    "        # Load model\n",
    "        pose_model_loader = PoseModelLoader()\n",
    "        det_model_loader = YoloLoader()\n",
    "            \n",
    "        cap = cv2.VideoCapture(self.videoPath)\n",
    "        fI_jump=0\n",
    "        fIList=[]\n",
    "        personNumList=[]\n",
    "        resultList=[]\n",
    "        for fI in tqdm(range(0,self.fcount,self.fps*by_sec)):\n",
    "            #if fI<fI_jump:\n",
    "                #continue\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES,fI)\n",
    "            ret, frame = cap.read()\n",
    "            framePose = FrameProcessor(frame, det_model_loader, pose_model_loader).start()\n",
    "            result = framePose.resultNew\n",
    "            personNum = framePose.det_human_num\n",
    "            personNumList.append(personNum)\n",
    "            resultList.append(result)\n",
    "            fIList.append(fI)\n",
    "            #fI_jump = fI + self.fps*filter_jump_sec\n",
    "        \n",
    "        personNumThre=int(0.9*max(personNumList))\n",
    "        for fI, personNum, result in tqdm(zip(fIList, personNumList, resultList)):\n",
    "            if personNum<personNumThre:\n",
    "                continue\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES,fI)\n",
    "            ret, frame = cap.read()\n",
    "            imgSavName=self.videoname[:-4]+'_'+str(fI)+'_'+str(personNum)+'.jpg'\n",
    "            jsonSavName=self.videoname[:-4]+'_'+str(fI)+'_'+str(personNum)+'_keypoints.json'\n",
    "            sav_img(os.path.join(imgSavDir, imgSavName), frame)\n",
    "            with open(os.path.join(jsonSavDir, jsonSavName),'w',encoding='utf-8') as jsonfile:\n",
    "                json.dump(result, jsonfile)\n",
    "        cap.release() \n",
    "\n",
    "videoPathList = glob.glob(r'/home/ubuntu/Data/Video/class/*.mp4')\n",
    "for videoPath in videoPathList:\n",
    "    testVideo = myVideoLoader(videoPath)\n",
    "    testVideo.auto_extract_frame('/home/ubuntu/Data/autoFrameFilter/',by_sec=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "videoPathList = glob.glob."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pose model from ./models/sppe/duc_se.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/modules/upsampling.py:129: UserWarning: nn.Upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.{} is deprecated. Use nn.functional.interpolate instead.\".format(self.name))\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:2351: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import cv2\n",
    "from myutil import *\n",
    "\n",
    "from dataloader import FrameProcessor, YoloLoader, PoseModelLoader\n",
    "def draw_one_alphapose(frame, oneResult, format='coco'):\n",
    "    if format == 'coco':\n",
    "        l_pair = [(0, 1), (0, 2), (1, 3), (2, 4),  # Head\n",
    "            (5, 6), (5, 7), (7, 9), (6, 8), (8, 10),(17, 11), (17, 12),  # Body\n",
    "            (11, 13), (12, 14), (13, 15), (14, 16)]\n",
    "        p_color = [(0, 255, 255),(0, 191, 255),(0, 255, 102),(0, 77, 255), (0, 255, 0), \n",
    "                   #Nose, LEye, REye, LEar, REar\n",
    "                  (77,255,255),(77, 255, 204), (77,204,255), (191, 255, 77), (77,191,255), (191, 255, 77), \n",
    "                   #LShoulder, RShoulder, LElbow, RElbow, LWrist, RWrist\n",
    "                  (204,77,255),(77,255,204), (191,77,255), (77,255,191), (127,77,255), (77,255,127), (0, 255, 255)] \n",
    "                   #LHip, RHip, LKnee, Rknee, LAnkle, RAnkle, Neck\n",
    "        line_color = [(0, 215, 255), (0, 255, 204), (0, 134, 255), (0, 255, 50), \n",
    "                    (77,255,222), (77,196,255), (77,135,255), (191,255,77), (77,255,77), \n",
    "                    (77,222,255), (255,156,127), \n",
    "                    (0,127,255), (255,127,77), (0,77,255), (255,77,36)]\n",
    "    img = frame\n",
    "    part_line = {}\n",
    "    kp_preds = np.array(oneResult['keypoints'])\n",
    "    kp_scores = np.array(oneResult['kp_score'])\n",
    "    # Draw keypoints\n",
    "    for n in range(len(kp_scores)):\n",
    "        if kp_scores[n] <= 0.05:\n",
    "            continue\n",
    "        cor_x, cor_y = int(kp_preds[n, 0]), int(kp_preds[n, 1])\n",
    "        part_line[n] = (cor_x, cor_y)\n",
    "        cv2.circle(img, (cor_x, cor_y), 4, p_color[n], -1)\n",
    "    # Draw limbs\n",
    "    for i, (start_p, end_p) in enumerate(l_pair):\n",
    "        if start_p in part_line and end_p in part_line:\n",
    "            start_xy = part_line[start_p]\n",
    "            end_xy = part_line[end_p]\n",
    "            cv2.line(img, start_xy, end_xy, line_color[i], 2*(kp_scores[start_p] + kp_scores[end_p]) + 1)\n",
    "    return img\n",
    "\n",
    "\n",
    "\n",
    "imgPath = '/home/ubuntu/Data/IMG/4.JPG'\n",
    "frame = cv2.imread(imgPath)\n",
    "\n",
    "# Load pose model\n",
    "pose_model_loader = PoseModelLoader()\n",
    "det_model_loader = YoloLoader()\n",
    "startTime = time.time()\n",
    "framePose = FrameProcessor(frame, det_model_loader, pose_model_loader).start()\n",
    "#framePose.out_img\n",
    "#print(time.time()-startTime)\n",
    "#print(framePose.person_num, framePose.out_img.shape)\n",
    "#print(framePose.result)\n",
    "#sav_img( '/home/ubuntu/Data/output/test.JPG',out_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_img(img, mode=None):\n",
    "    if mode=='BGR':\n",
    "        img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "    plt.imshow(img)\n",
    "    plt.show() \n",
    "plt_img(framePose.out_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_img = draw_one_alphapose(frame, framePose.resultNew[0])\n",
    "sav_img( '/home/ubuntu/Data/output/test_3st.JPG',out_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.79628235]\n",
      " [0.81959236]\n",
      " [0.80907679]\n",
      " [0.30262834]\n",
      " [0.60456681]\n",
      " [0.69666219]\n",
      " [0.63703501]\n",
      " [0.76484483]\n",
      " [0.77513301]\n",
      " [0.79137766]\n",
      " [0.68912852]\n",
      " [0.3997393 ]\n",
      " [0.43122238]\n",
      " [0.20353107]\n",
      " [0.10328924]\n",
      " [0.19517055]\n",
      " [0.15587234]\n",
      " [0.6668486 ]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "for human in framePose.resultNew:\n",
    "    print(np.array(human['kp_score']))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=[1,2,3,4,5,6]\n",
    "max(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [00:00, 22389.52it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "aList=[1,2,3,4,5,6]\n",
    "bList=[1,2,3,4,5,6]\n",
    "for a, b in tqdm(zip(aList,bList)):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(0.95*max(aList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-4d3fa39e6c4f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'keypoints'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvideoname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mjsonSavname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "os.path.join('/home','keypoints',self.videoname[:-4],jsonSavname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
